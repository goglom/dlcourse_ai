{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Neural Network.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jXM9Dz-Wzag5"},"source":["# Задание 2.1 - Нейронные сети\n","\n","В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n","\n","В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n","\n","<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsLJyjc60OWv","executionInfo":{"status":"ok","timestamp":1617272855457,"user_tz":-420,"elapsed":94836,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}},"outputId":"62adac9c-baa7-4485-b9d9-77a54f56fc1d"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)\n","assignment_dir = \"/content/drive/MyDrive/dlcourse_ai-master/assignments/assignment2\"\n","os.chdir(assignment_dir)\n","assert os.getcwd() == assignment_dir"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RoG89-FFXD8h"},"source":["** Вопросы**\n","\n","1. **Empty**"]},{"cell_type":"code","metadata":{"id":"5gZRItavzahF","executionInfo":{"status":"ok","timestamp":1617272855459,"user_tz":-420,"elapsed":94823,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KAK7I3dzahH","executionInfo":{"status":"ok","timestamp":1617272858977,"user_tz":-420,"elapsed":98333,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["from dataset import load_svhn, random_split_train_val\n","from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n","from layers import FullyConnectedLayer, ReLULayer\n","from model import TwoLayerNet\n","from trainer import Trainer, Dataset\n","from optim import SGD, MomentumSGD\n","from metrics import multiclass_accuracy"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"muzK3kC9zahM"},"source":["# Загружаем данные\n","\n","И разделяем их на training и validation."]},{"cell_type":"code","metadata":{"id":"LAqxapZGzahM","executionInfo":{"status":"ok","timestamp":1617272877198,"user_tz":-420,"elapsed":116542,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["def prepare_for_neural_network(train_X, test_X):\n","    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n","    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n","    \n","    # Subtract mean\n","    mean_image = np.mean(train_flat, axis = 0)\n","    train_flat -= mean_image\n","    test_flat -= mean_image\n","    \n","    return train_flat, test_flat\n","    \n","train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n","train_X, test_X = prepare_for_neural_network(train_X, test_X)\n","# Split train into train and val\n","train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mdll-g3lzahO"},"source":["# Как всегда, начинаем с кирпичиков\n","\n","Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n","- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n","- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n","\n","Начнем с ReLU, у которого параметров нет."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"PalUkJAvzahO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617272877199,"user_tz":-420,"elapsed":116354,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}},"outputId":"5effae7e-a860-4fd6-f67e-22fc4c35b1db"},"source":["# TODO: Implement ReLULayer layer in layers.py\n","# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n","\n","X = np.array([[1,-2,3],\n","              [-1, 2, 0.1]\n","              ])\n","\n","assert check_layer_gradient(ReLULayer(), X)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Gradient check passed!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4uSwHHXizahP"},"source":["А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n","\n","Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n","\n","Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."]},{"cell_type":"code","metadata":{"id":"4v0DddOMzahQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617272877201,"user_tz":-420,"elapsed":116339,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}},"outputId":"4dbb7d93-deaa-42bb-e200-b0c4cca82b53"},"source":["# TODO: Implement FullyConnected layer forward and backward methods\n","assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n","# TODO: Implement storing gradients for W and B\n","assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n","assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Gradient check passed!\n","Gradient check passed!\n","Gradient check passed!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DyBYEx6XzahR"},"source":["## Создаем нейронную сеть\n","\n","Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n","\n","Не забудьте реализовать очистку градиентов в начале функции."]},{"cell_type":"code","metadata":{"id":"8EDTtnD0zahS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617272884941,"user_tz":-420,"elapsed":124065,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}},"outputId":"fece5549-90c3-4cdd-8229-e2cdad05e2a8"},"source":["# TODO: In model.py, implement compute_loss_and_gradients function\n","model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n","loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n","\n","# TODO Now implement backward pass and aggregate all of the params\n","check_model_gradient(model, train_X[:2], train_y[:2])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Checking gradient for W1\n","Gradient check passed!\n","Checking gradient for B1\n","Gradient check passed!\n","Checking gradient for W2\n","Gradient check passed!\n","Checking gradient for B2\n","Gradient check passed!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Sv1XfnSBzahU"},"source":["Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."]},{"cell_type":"code","metadata":{"id":"9gFXQVayzahU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617272892146,"user_tz":-420,"elapsed":131266,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}},"outputId":"c5b9ff12-c068-4b7f-b1ba-4e369f6dd137"},"source":["# TODO Now implement l2 regularization in the forward and backward pass\n","model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n","loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n","assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n","    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n","\n","check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Checking gradient for W1\n","Gradient check passed!\n","Checking gradient for B1\n","Gradient check passed!\n","Checking gradient for W2\n","Gradient check passed!\n","Checking gradient for B2\n","Gradient check passed!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"1NVbuFkBzahV"},"source":["Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n","\n","Какое значение точности мы ожидаем увидеть до начала тренировки?"]},{"cell_type":"code","metadata":{"id":"mY6GubkLzahV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617272892147,"user_tz":-420,"elapsed":131222,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}},"outputId":"a751dafb-e5e5-48cf-b629-e0e382f113cd"},"source":["# Finally, implement predict function!\n","\n","# TODO: Implement predict function\n","# What would be the value we expect?\n","multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"657KwcQKzahV"},"source":["# Допишем код для процесса тренировки\n","\n","Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."]},{"cell_type":"code","metadata":{"id":"wmmi53HvzahW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617274017815,"user_tz":-420,"elapsed":575285,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}},"outputId":"76b3f70f-61ca-43b4-b063-b3ae87795fe3"},"source":["model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n","dataset = Dataset(train_X, train_y, val_X, val_y)\n","trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2, do_print=True)\n","\n","# TODO Implement missing pieces in Trainer.fit function\n","# You should expect loss to go down every epoch, even if it's slow\n","loss_history, train_history, val_history = trainer.fit()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch: 0; Loss: 2.321342, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 1; Loss: 2.302289, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 2; Loss: 2.302285, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 3; Loss: 2.302295, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 4; Loss: 2.302310, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 5; Loss: 2.302294, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 6; Loss: 2.302300, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 7; Loss: 2.302291, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 8; Loss: 2.302296, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 9; Loss: 2.302284, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 10; Loss: 2.302299, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 11; Loss: 2.302301, Train accuracy: 0.148222, val accuracy:0.140000\n","Epoch: 12; Loss: 2.302300, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 13; Loss: 2.302309, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 14; Loss: 2.302296, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 15; Loss: 2.302290, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 16; Loss: 2.302300, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 17; Loss: 2.302298, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 18; Loss: 2.302281, Train accuracy: 0.196667, val accuracy:0.206000\n","Epoch: 19; Loss: 2.302290, Train accuracy: 0.196667, val accuracy:0.206000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rVTOjC20zahW","executionInfo":{"status":"aborted","timestamp":1617273414696,"user_tz":-420,"elapsed":653757,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["plt.figure(figsize=(16, 5), dpi=100)\n","plt.subplot(1, 2, 1)\n","plt.plot(train_history, label=\"train\")\n","plt.plot(val_history, label=\"validation\")\n","plt.legend()\n","plt.grid()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(loss_history, label=\"loss history\")\n","plt.legend()\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qcERsg_zahW"},"source":["# Улучшаем процесс тренировки\n","\n","Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."]},{"cell_type":"markdown","metadata":{"id":"--SHT-YCzahX"},"source":["## Уменьшение скорости обучения (learning rate decay)\n","\n","Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n","\n","Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n","\n","В нашем случае N будет равным 1."]},{"cell_type":"code","metadata":{"id":"f9ycs2WszahX","executionInfo":{"status":"aborted","timestamp":1617273414701,"user_tz":-420,"elapsed":653757,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["# TODO Implement learning rate decay inside Trainer.fit method\n","# Decay should happen once per epoch\n","\n","model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 50, reg = 1e-1)\n","dataset = Dataset(train_X, train_y, val_X, val_y)\n","trainer = Trainer(model, dataset, SGD(), batch_size=15, learning_rate_decay=0.99)\n","\n","initial_learning_rate = trainer.learning_rate\n","loss_history, train_history, val_history = trainer.fit()\n","\n","assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n","assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtiikZcozahX"},"source":["# Накопление импульса (Momentum SGD)\n","\n","Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n","\n","Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n","(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n","\n","```\n","velocity = momentum * velocity - learning_rate * gradient \n","w = w + velocity\n","```\n","\n","`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n","\n","Несколько полезных ссылок, где метод разбирается более подробно:  \n","http://cs231n.github.io/neural-networks-3/#sgd  \n","https://distill.pub/2017/momentum/"]},{"cell_type":"code","metadata":{"id":"UhNioGaizahZ","executionInfo":{"status":"aborted","timestamp":1617273414703,"user_tz":-420,"elapsed":653750,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["# TODO: Implement MomentumSGD.update function in optim.py\n","\n","model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n","dataset = Dataset(train_X, train_y, val_X, val_y)\n","trainer = Trainer(model, dataset, MomentumSGD(), do_print=True, learning_rate=1e-4, learning_rate_decay=0.99)\n","\n","# You should see even better results than before!\n","loss_history, train_history, val_history = trainer.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lpE9oOvizahZ"},"source":["# Ну что, давайте уже тренировать сеть!"]},{"cell_type":"markdown","metadata":{"id":"8nJCtjJTzaha"},"source":["## Последний тест - переобучимся (overfit) на маленьком наборе данных\n","\n","Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n","Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n","\n","Если этого не происходит, то где-то была допущена ошибка!"]},{"cell_type":"code","metadata":{"id":"_qioxSs7zahb","executionInfo":{"status":"aborted","timestamp":1617273414705,"user_tz":-420,"elapsed":653747,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["data_size = 15\n","model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n","dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n","trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n","\n","# You should expect this to reach 1.0 training accuracy \n","loss_history, train_history, val_history = trainer.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XfywOEgMzahb"},"source":["Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n","Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n","Найдите их!"]},{"cell_type":"code","metadata":{"id":"m3zGt-nOzahc","executionInfo":{"status":"aborted","timestamp":1617273414706,"user_tz":-420,"elapsed":653738,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n","\n","model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n","dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n","# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n","trainer = Trainer(model, dataset, MomentumSGD(), \n","                  learning_rate=0.1, \n","                  learning_rate_decay=0.99, \n","                  num_epochs=20, batch_size=5\n","                  )\n","\n","loss_history, train_history, val_history = trainer.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHjniOaSzahd"},"source":["# Итак, основное мероприятие!\n","\n","Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n","\n","Добейтесь точности лучше **60%** на validation set."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"eH1RsU6Bzahe","executionInfo":{"status":"aborted","timestamp":1617273414707,"user_tz":-420,"elapsed":653731,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["# Let's train the best one-hidden-layer network we can\n","\n","learning_rates = [1e-1]\n","reg_strengths = [1e-3]\n","learning_rate_decays = [0.99]\n","hidden_layer_sizes = [512]\n","nums_epochs = [128]\n","batch_sizes = [256]\n","dataset = Dataset(train_X, train_y, val_X, val_y)\n","\n","best_classifier = None\n","best_val_accuracy = 0.0\n","\n","best_params = []\n","best_loss_history = []\n","best_train_history = []\n","best_val_history = []\n","\n","for learning_rate in learning_rates:\n","    for reg_strength in reg_strengths:\n","        for learning_rate_decay in learning_rate_decays:\n","            for hidden_layer_size in hidden_layer_sizes:\n","                for num_epochs in nums_epochs:\n","                    for batch_size in batch_sizes:\n","                        model = TwoLayerNet(n_input=train_X.shape[1], \n","                                            n_output=10, \n","                                            hidden_layer_size=hidden_layer_size, \n","                                            reg=reg_strength\n","                                            )\n","                        trainer = Trainer(model, dataset, MomentumSGD(), \n","                                          learning_rate=learning_rate, \n","                                          learning_rate_decay=learning_rate_decay, \n","                                          num_epochs=num_epochs,\n","                                          batch_size=batch_size,\n","                                          do_print=True\n","                                          )\n","                        loss_history, train_history, val_history = trainer.fit()\n","                        val_acc = val_history[-1]\n","                        print(f'validation acc = {val_acc}')\n","                        if (val_acc > best_val_accuracy):\n","                            best_params = [\n","                                           learning_rate,\n","                                           reg_strength,\n","                                           learning_rate_decay,\n","                                           hidden_layer_size,\n","                                           num_epochs,\n","                                           batch_size\n","                            ]\n","                            best_val_accuracy = val_acc\n","                            best_classifier = model\n","                            best_loss_history = loss_history\n","                            best_train_history = train_history\n","                            best_val_history = val_history\n","\n","\n","print('best validation accuracy achieved: %f' % best_val_accuracy)\n","print(f'best params: {best_params}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mahdaIoEzahe","executionInfo":{"status":"aborted","timestamp":1617273414708,"user_tz":-420,"elapsed":653719,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["plt.figure(figsize=(15, 7))\n","plt.subplot(211)\n","plt.title(\"Loss\")\n","plt.plot(loss_history)\n","plt.subplot(212)\n","plt.title(\"Train/validation accuracy\")\n","plt.plot(train_history)\n","plt.plot(val_history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QQUimSiBzahe"},"source":["# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"]},{"cell_type":"code","metadata":{"id":"rc4n1BYNzahf","executionInfo":{"status":"aborted","timestamp":1617273414709,"user_tz":-420,"elapsed":653709,"user":{"displayName":"Вадим Дмитриев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIeFj_kZjZej_FdpM_5HMuZ-JycxiaMkDpn12y=s64","userId":"15273218268132906197"}}},"source":["test_pred = best_classifier.predict(test_X)\n","test_accuracy = multiclass_accuracy(test_pred, test_y)\n","print('Neural net test set accuracy: %f' % (test_accuracy, ))"],"execution_count":null,"outputs":[]}]}